---
title : 2023年adventカレンダー記事(aktm)
date : 2023-12-20
author : aketami
---
こんにちは20日の担当のakrmです。

皆さんはChatGPTなどの大規模言語モデル（LLM）を使ったことがありますか？ 私はリリース当初からGPT系のツールをよく利用しています。もともと何でも自動化するのが好きで、時間を節約するためのシステムを作ることに夢中になってきました。もっとも、システムの構築にかかる時間のほうが短縮した時間を上回ることが多いですが…。

それでもChatGPTをはじめとするLLMは、私にとって非常に魅力的なツールです。膨大なテキストデータで学習されており、多種多様な質問や指示に対応できるからです。ただ、学習範囲に含まれていない専門的な分野の質問などは、LLM単体では答えが難しいこともあります。

そこで活用されるのが、**embedding（埋め込み）**と呼ばれる手法です。質問をLLMに投げたあと、解答に必要な文書を検索し、その情報をもとに回答を組み立てるというものです。ポイントは、文書を数値のベクトルとして表現して検索することにあります。

### ベクトル化と単語の意味の捉え方

テキストのベクトル化では、単語や文章を多次元空間の点として扱います。たとえば「王女」「王」「女」「男」のような単語は、それぞれ次元数1024のベクトルで表されるとイメージしてください。  
このとき、単語同士の意味的な関係は、ベクトルの「足し算」や「引き算」で近似的に表すことができます。  
よく知られた例として、  
v_{王女} = v_{王} + v_{女} - v_{男}  
があります。これは、「王女」の意味が「王」と「女」を足して「男」を引いたあたりで表されるということを示す式です。

### Google検索とembedding

実は、このembedding技術を拡張して文単位で行うことによって、Google検索などでも応用されています。大きく分けると以下の2つの目的で使われています。

1. **検索クエリの意図を把握する**  
   ユーザーが入力した単語やフレーズの微妙な違いをとらえ、文脈に合わせて最適な検索結果を提示します。

2. **文書のベクトル化とランキング**  
   ウェブページやドキュメントもベクトル化しておき、クエリとの類似度をもとに検索結果を並び替えます。これによって、単純なキーワードの一致だけでなく、より「意味的に近い」ページが上位に表示されやすくなります。

こうした技術のおかげで、Google検索は単なるキーワードマッチングのツールから、一歩進んだ意図理解型の検索エンジンに進化しています。

### embeddingを用いた回答生成の流れ

LLMとembeddingを組み合わせて、より正確な回答を得るには、主に次のステップを踏みます。

1. 回答の参考となる文書をあらかじめ分割し、ベクトル化しておく。  
2. ユーザーからの質問もベクトル化する。  
3. 質問ベクトルとの距離が近い文書を検索する。  
4. 見つかった文書を参照しつつ、LLMに最終的な回答を生成させる。

最近のアップデートでは、この一連のプロセスをWeb上で完結できる機能も登場しています。OpenAIの「Create a GPTs」では、参考文書としてPDFなどを登録しておけば、自動的にembeddingを行い、LLMが回答に活用してくれます。

実際に2023年度のJudge Handbookを読み込ませて質問すると、その内容に即した回答が得られました。まだ「どのページを参照したのか」といった詳細には答えづらい部分もありますが、今後の改善が期待されるところです。
{{< figure src="https://angry-vanadium-ceb.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F488ae9e4-91fb-44da-b02a-428688e6feaa%2F6cb0b1d5-114b-47ad-b248-f66756971178%2F%25E3%2582%25B9%25E3%2582%25AF%25E3%2583%25AA%25E3%2583%25BC%25E3%2583%25B3%25E3%2582%25B7%25E3%2583%25A7%25E3%2583%2583%25E3%2583%2588_2023-12-19_19.06.41.png?table=block&id=56cf972d-f1a1-46f3-8041-8652db772ae2&spaceId=488ae9e4-91fb-44da-b02a-428688e6feaa&width=2000&userId=&cache=v2" title="" width="500px">}}

### embedding手法の課題とLLMの役割

embeddingを使った手法は、既に持っている文書や単語の関係性から答えを導くのが得意です。しかし、新規の質問や独自のコンテキストを必要とするケースでは、うまく対応できないこともあります。そうしたときは、まずLLMが質問内容を深く理解し、どの文書を参照すべきかを特定し、そこからembeddingによる検索をかけるといった組み合わせが有効です。

### 今後の展望：合成生物学への応用

私はこの技術を応用して、プラスミドやBioBricksの設計を会話形式で行えるシステムを構想しています。すでに数百のpartsをインデックス化した仕組みは作ってありますが、データの質があまり良くないため、まだ思ったようには動いていません。

それでも、このプロジェクトには大きな可能性があると感じています。専門外の人でも、自然言語で会話しながらiGEMに必要な分子生物学的アプローチを学んだり設計できたりすれば、もっと気軽にiGEMに参加できるようになるでしょう。iGEMが掲げる「さまざまなバックグラウンドを持つ人が合成生物学のコミュニティに参加して発展させる」という理念にも合致していると思います。

---

全体として、LLMとembeddingの組み合わせは非常に強力で、活用範囲がどんどん広がっています。技術の進歩でより正確かつ柔軟な回答が得られるようになれば、私たちの生活や研究がさらに便利になることは間違いありません。私も引き続き、LLMを活用したさまざまなプロジェクトに挑戦していきたいと思います。